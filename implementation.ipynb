{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Implementation: Iterative RAG vs Gold Context\n",
    "\n",
    "This notebook implements the diagnostic study from the paper **'When Iterative RAG Beats Ideal Evidence'**. We benchmark three regimes:\n",
    "1. **No Context**: Reliance on parametric memory.\n",
    "2. **Gold Context**: Access to oracle supporting evidence.\n",
    "3. **Iterative RAG**: A step-by-step retrieval and reasoning loop.\n",
    "\n",
    "We use **HotpotQA** as a proxy for multi-hop scientific QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install -q transformers sentence-transformers datasets faiss-cpu seaborn matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Config\n",
    "MODEL_NAME = 'google/flan-t5-base'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "DATASET_NAME = 'hotpot_qa'\n",
    "DATASET_CONFIG = 'distractor'\n",
    "SAMPLE_SIZE = 50 # Adjusted for reasonable runtime in notebook\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieval & Agent Classes\n",
    "\n",
    "We define the `DenseRetriever` using FAISS and `RAGController` which manages the iterative generation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseRetriever:\n",
    "    def __init__(self, embedding_model_name):\n",
    "        self.encoder = SentenceTransformer(embedding_model_name, device=DEVICE)\n",
    "        self.index = None\n",
    "        self.docs = []\n",
    "    \n",
    "    def build_index(self, corpus_texts):\n",
    "        self.docs = corpus_texts\n",
    "        embeddings = self.encoder.encode(corpus_texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        self.index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "    def retrieve(self, query, k=1):\n",
    "        q_embed = self.encoder.encode([query], convert_to_numpy=True)\n",
    "        q_embed = q_embed / np.linalg.norm(q_embed, axis=1, keepdims=True)\n",
    "        D, I = self.index.search(q_embed, k)\n",
    "        return [self.docs[i] for i in I[0]], D[0]\n",
    "\n",
    "class RAGController:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(DEVICE)\n",
    "        \n",
    "    def generate(self, prompt, max_new_tokens=64):\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1024).to(DEVICE)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=0.0)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def run_no_context(self, question):\n",
    "        return self.generate(f'Question: {question}\\nAnswer:')\n",
    "\n",
    "    def run_gold_context(self, question, gold_facts):\n",
    "        context_str = ' '.join(gold_facts)\n",
    "        return self.generate(f'Context: {context_str}\\nQuestion: {question}\\nAnswer:')\n",
    "\n",
    "    def run_iterative_rag(self, question, retriever, max_steps=3):\n",
    "        current_context = []\n",
    "        trajectory = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            context_str = ' '.join(current_context)\n",
    "            # Dynamic Prompting based on current state\n",
    "            if step == 0:\n",
    "                reasoning_prompt = f'Question: {question}\\nTask: Identify the first entity or fact needed to answer this question. Output a search query.'\n",
    "            else:\n",
    "                reasoning_prompt = f'Question: {question}\\nKnown Facts: {context_str}\\nTask: What information is still missing? Output a search query.'\n",
    "            \n",
    "            search_query = self.generate(reasoning_prompt, max_new_tokens=32)\n",
    "            docs, scores = retriever.retrieve(search_query, k=1)\n",
    "            retrieved_doc = docs[0]\n",
    "            \n",
    "            if retrieved_doc not in current_context:\n",
    "                current_context.append(retrieved_doc)\n",
    "                \n",
    "            trajectory.append(search_query)\n",
    "            \n",
    "            # Simple stopping heuristic\n",
    "            check_prompt = f'Question: {question}\\nContext: {\' \'.join(current_context)}\\nCan you answer the question based on the context? Answer yes or no.'\n",
    "            if 'yes' in self.generate(check_prompt, max_new_tokens=5).lower():\n",
    "                break\n",
    "        \n",
    "        final_answer = self.generate(f'Context: {\' \'.join(current_context)}\\nQuestion: {question}\\nAnswer:')\n",
    "        return final_answer, trajectory, current_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Utilities\n",
    "\n",
    "Standard normalization and Exact Match (EM) scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text): return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text): return ' '.join(text.split())\n",
    "    def remove_punc(text): return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the Benchmark\n",
    "\n",
    "We load a slice of HotpotQA validation set. For each question, we build a temporary FAISS index containing the provided context (gold + distractors) to simulate the retrieval pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dataset = load_dataset(DATASET_NAME, DATASET_CONFIG, split='validation', trust_remote_code=True)\n",
    "dataset = dataset.select(range(SAMPLE_SIZE))\n",
    "\n",
    "# Init Models\n",
    "retriever_model = DenseRetriever(EMBEDDING_MODEL)\n",
    "agent = RAGController(MODEL_NAME)\n",
    "\n",
    "results = []\n",
    "print(f'Starting benchmark on {SAMPLE_SIZE} samples...')\n",
    "\n",
    "for idx, item in tqdm(enumerate(dataset), total=SAMPLE_SIZE):\n",
    "    question = item['question']\n",
    "    answer = item['answer']\n",
    "    \n",
    "    # Preprocess corpus for this question\n",
    "    corpus_texts = []\n",
    "    gold_facts = []\n",
    "    sup_facts_set = set([(f[0], f[1]) for f in item['supporting_facts']])\n",
    "    \n",
    "    for title, sentences in zip(item['context']['title'], item['context']['sentences']):\n",
    "        for i, sent in enumerate(sentences):\n",
    "            full_sent = f'{title}: {sent}'\n",
    "            corpus_texts.append(full_sent)\n",
    "            if (title, i) in sup_facts_set:\n",
    "                gold_facts.append(full_sent)\n",
    "                \n",
    "    retriever_model.build_index(corpus_texts)\n",
    "    \n",
    "    # Run Regimes\n",
    "    ans_no_ctx = agent.run_no_context(question)\n",
    "    ans_gold = agent.run_gold_context(question, gold_facts)\n",
    "    ans_iter, traj, final_ctx = agent.run_iterative_rag(question, retriever_model)\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    em_no_ctx = exact_match_score(ans_no_ctx, answer)\n",
    "    em_gold = exact_match_score(ans_gold, answer)\n",
    "    em_iter = exact_match_score(ans_iter, answer)\n",
    "    \n",
    "    gold_set = set(gold_facts)\n",
    "    retrieved_set = set(final_ctx)\n",
    "    recall = len(gold_set.intersection(retrieved_set)) / len(gold_set) if len(gold_set) > 0 else 0\n",
    "    \n",
    "    results.append({\n",
    "        'no_context_em': em_no_ctx,\n",
    "        'gold_em': em_gold,\n",
    "        'iterative_em': em_iter,\n",
    "        'iterative_recall': recall\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis and Visualization\n",
    "\n",
    "We visualize the Exact Match (EM) accuracy across the three regimes and analyze the effectiveness of the Iterative Retrieval in recovering gold evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Accuracy Comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "means = df_results[['no_context_em', 'gold_em', 'iterative_em']].mean()\n",
    "sns.barplot(x=means.index, y=means.values, palette='viridis')\n",
    "plt.title('Performance Comparison: Iterative RAG vs Baselines')\n",
    "plt.ylabel('Exact Match Accuracy')\n",
    "plt.xlabel('Regime')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Evidence Recall\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df_results['iterative_recall'], bins=5, kde=True, color='orange')\n",
    "plt.title('Diagnostic: Gold Evidence Recall in Iterative RAG')\n",
    "plt.xlabel('Recall (Fraction of Gold Facts Retrieved)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Table\n",
    "print(df_results.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}